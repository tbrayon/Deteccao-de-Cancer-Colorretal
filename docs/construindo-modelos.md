# Prepara√ß√£o dos dados

<p align="justify">No contexto de projetos de an√°lise de dados e constru√ß√£o de modelos preditivos, a etapa de prepara√ß√£o dos dados assume um papel fundamental. O c√≥digo apresentado demonstra um fluxo de trabalho bem estruturado para o pr√©-processamento de um conjunto de dados possivelmente relacionado √† previs√£o de c√¢ncer colorretal. Cada etapa visa garantir que os modelos subsequentes recebam dados limpos, tratados e em um formato adequado para o aprendizado eficaz.</p>

(Nesta etapa, dever√£o ser descritas todas as t√©cnicas utilizadas para pr√©-processamento/tratamento dos dados.)

(Algumas das etapas podem estar relacionadas √†:)

## 1. Separa√ß√£o de features e targets:

<p align="justify">Nesta etapa incial √© feito a separa√ß√£o das colunas features (colunas gerais do dataset) e das colunas targets (colunas alvo a qual trat√° as respostas. </p>

```python
targets = ['Survival_Status', 'Chemotherapy_Received', 'Radiotherapy_Received', 'Surgery_Received']
# O restante dos dados em features (excluindo as colunas de targets) s√£o as vari√°veis explicativas ou inputs usadas como base para
# realizar as previs√µes.
features = df.drop(columns=targets)
```

## 2. Convers√£o e tratamento de valores num√©ricos e categ√≥ricos (Imputa√ß√£o):

<p align="justify"> Identifica√ß√£o de tipos de dados os quais foram separados em dois grupos, num√©ricos e categ√≥ricos. </p>

```python
# colunas com valores num√©ricos (ex.: idade, IMC).
numeric_features = features.select_dtypes(include=np.number).columns.tolist()
# colunas com valores categ√≥ricos/texto (ex.: g√™nero, ra√ßa, regi√£o).
categorical_features = features.select_dtypes(exclude=np.number).columns.tolist()
```

No c√≥digo, a classe `SimpleImputer` do `scikit-learn` √© utilizada com estrat√©gias diferentes para vari√°veis num√©ricas e categ√≥ricas:

* **Vari√°veis num√©ricas:** utiliza-se a **mediana**, uma medida robusta √† presen√ßa de outliers.
* **Vari√°veis categ√≥ricas:** aplica-se a **moda** (valor mais frequente), preservando a categoria mais representativa.

```python
# Criar pipelines de pr√©-processamento
# Dados Num√©ricos
numeric_transformer = Pipeline(steps=[
     # Substitui valores faltantes pela mediana dos dados
    ('imputer', SimpleImputer(strategy='median')), # Trata valores ausentes em colunas num√©ricas
     # Converte os valores num√©ricos para uma escala comum (m√©dia 0 e desvio padr√£o 1) com o StandardScaler. Isso √© importante para
     # garantir que todas as vari√°veis num√©ricas sejam tratadas igualmente pelos modelos.
    ('scaler', StandardScaler())])

# Dados Categ√≥ricos
categorical_transformer = Pipeline(steps=[
    # Substitui valores faltantes com o valor mais frequente em cada categoria
    ('imputer', SimpleImputer(strategy='most_frequent')), # Trata valores ausentes em colunas categ√≥ricas
    # Converte valores de texto (categorias) em vari√°veis num√©ricas bin√°rias (0 ou 1) usando o OneHotEncoder. Isso permite que os
    # modelos entendam as informa√ß√µes categ√≥ricas. Em situa√ß√µes como codifica√ß√£o "One-Hot" (quando convertemos categorias em
    # vari√°veis bin√°rias), frequentemente temos matrizes esparsas,pois cada categoria corresponde a um √∫nico 1 em um vetor cheio de 0s.
    ('onehot', OneHotEncoder(handle_unknown='ignore'))])

# Criar o preprocessor usando ColumnTransformer. Combina√ß√£o dos Preprocessamentos. Isso organiza o preprocessamento de todas as colunas
# de forma eficiente. O ColumnTransformer aplica:
preprocessor = ColumnTransformer(
    transformers=[
        # O pipeline de dados num√©ricos (numeric_transformer) nas colunas definidas como numeric_features.
        ('num', numeric_transformer, numeric_features),
        # O pipeline de dados categ√≥ricos (categorical_transformer) nas colunas definidas como categorical_features.
        ('cat', categorical_transformer, categorical_features)])

# Ajustar e transformar os dados. Transforma√ß√£o e Cria√ß√£o de DataFrame. Aplica os pipelines (num√©ricos e categ√≥ricos) em todo
# o conjunto de dados features (colunas exceto as targets), retornando uma matriz contendo os dados pr√©-processados.
preprocessed_data = preprocessor.fit_transform(features)

# Extra√ß√£o dos Nomes das Vari√°veis.
# numeric_names: Mant√©m os nomes originais das colunas num√©ricas.
numeric_names = numeric_features
# categorical_names: Extrai os nomes das vari√°veis criadas pelo OneHotEncoder (ex.: Gender_Male, Gender_Female).
categorical_names = preprocessor.named_transformers_['cat'].named_steps['onehot']\
                   .get_feature_names_out(categorical_features)
# all_feature_names: Junta os nomes das colunas num√©ricas e categ√≥ricas em um √∫nico array.
all_feature_names = np.concatenate([numeric_names, categorical_names])

# Convers√£o para DataFrame para an√°lise (se sparse matrix). Os dados preprocessados s√£o convertidos em um DataFrame do pandas para
# facilitar a an√°lise. Se a matriz resultante for esparsa (√© uma estrutura na qual a maioria dos valores s√£o zeros. Ela √© utilizada
# para economizar mem√≥ria e tornar os c√°lculos mais eficientes.), ela ser√° convertida em um array denso (toarray() - armazena todos
# os valores explicitamente, incluindo os zeros. Este tipo de array n√£o otimiza mem√≥ria)
if hasattr(preprocessed_data, "toarray"):
    preprocessed_df = pd.DataFrame.sparse.from_spmatrix(preprocessed_data, columns=all_feature_names)
else:
    preprocessed_df = pd.DataFrame(preprocessed_data, columns=all_feature_names)

```
<p align="justify"> O tratamento de valores ausentes, como parte da limpeza de dados, est√° acontecendo dentro dos pipelines numeric_transformer e categorical_transformer, usando o SimpleImputer com diferentes estrat√©gias para colunas num√©ricas e categ√≥ricas. O ColumnTransformer aplica esses pipelines √†s colunas apropriadas, e o fit_transform executa a imputa√ß√£o durante a transforma√ß√£o dos dados. Esse tratamento evita a perda de dados relevantes e assegura que o conjunto final esteja completo e pronto para o treinamento dos modelos.</p>
<p align="justify">A presen√ßa de valores ausentes √© comum em bases de dados reais e pode comprometer significativamente o desempenho dos algoritmos de machine learning, j√° que a maioria deles n√£o lida diretamente com dados faltantes.</p>
<p align="justify"> Ap√≥s a imputa√ß√£o, os dados num√©ricos s√£o padronizados usando StandardScaler dentro do numeric_transformer. Essa etapa transforma os dados num√©ricos para que tenham m√©dia zero e desvio padr√£o um. Isso ajuda a evitar que vari√°veis com escalas diferentes dominem o modelo e melhora o desempenho de alguns algoritmos.</p>
<p align="justify">As vari√°veis categ√≥ricas s√£o transformadas usando OneHotEncoder dentro do categorical_transformer. Essa etapa converte cada categoria em uma nova coluna bin√°ria (0/1), evitando que o modelo interprete as categorias como tendo uma ordem intr√≠nseca.</p>
<p align="justify">O ColumnTransformer combina os pipelines numeric_transformer e categorical_transformer e os aplica √†s colunas apropriadas (numeric_features e categorical_features, respectivamente).</p>
<p align="justify">A fun√ß√£o fit_transform ajusta os pipelines aos dados de entrada (features) e, em seguida, transforma os dados aplicando todas as etapas de transforma√ß√£o definidas nos pipelines.</p>
<p align="justify">A transforma√ß√£o de dados envolve imputa√ß√£o de valores ausentes, padroniza√ß√£o de dados num√©ricos, codifica√ß√£o one-hot para dados categ√≥ricos e a aplica√ß√£o dessas transforma√ß√µes usando pipelines e ColumnTransformer. Essas etapas s√£o essenciais para preparar os dados do Datasete C√¢ncer Colorretal, garantindo que ele possa lidar com valores ausentes, diferentes escalas de vari√°veis e dados categ√≥ricos de forma eficaz.</p>

------
* Limpeza de Dados: trate valores ausentes: decida como lidar com dados faltantes, seja removendo linhas, preenchendo com m√©dias, medianas ou usando m√©todos mais avan√ßados; remova _outliers_: identifique e trate valores que se desviam significativamente da maioria dos dados.

* Transforma√ß√£o de Dados: normalize/padronize: torne os dados compar√°veis, normalizando ou padronizando os valores para uma escala espec√≠fica; codifique vari√°veis categ√≥ricas: converta vari√°veis categ√≥ricas em uma forma num√©rica, usando t√©cnicas como _one-hot encoding_.

* _Feature Engineering_: crie novos atributos que possam ser mais informativos para o modelo; selecione caracter√≠sticas relevantes e descarte as menos importantes.

* Tratamento de dados desbalanceados: se as classes de interesse forem desbalanceadas, considere t√©cnicas como _oversampling_, _undersampling_ ou o uso de algoritmos que lidam naturalmente com desbalanceamento.

* Separa√ß√£o de dados: divida os dados em conjuntos de treinamento, valida√ß√£o e teste para avaliar o desempenho do modelo de maneira adequada.
  
* Manuseio de Dados Temporais: se lidar com dados temporais, considere a ordena√ß√£o adequada e t√©cnicas espec√≠ficas para esse tipo de dado.
  
* Redu√ß√£o de Dimensionalidade: aplique t√©cnicas como PCA (An√°lise de Componentes Principais) se a dimensionalidade dos dados for muito alta.

* Valida√ß√£o Cruzada: utilize valida√ß√£o cruzada para avaliar o desempenho do modelo de forma mais robusta.

* Monitoramento Cont√≠nuo: atualize e adapte o pr√©-processamento conforme necess√°rio ao longo do tempo, especialmente se os dados ou as condi√ß√µes do problema mudarem.

* Entre outras....

Avalie quais etapas s√£o importantes para o contexto dos dados que voc√™ est√° trabalhando, pois a qualidade dos dados e a efic√°cia do pr√©-processamento desempenham um papel fundamental no sucesso de modelo(s) de aprendizado de m√°quina. √â importante entender o contexto do problema e ajustar as etapas de prepara√ß√£o de dados de acordo com as necessidades espec√≠ficas de cada projeto.

# Descri√ß√£o dos modelos

## Randon Forest:

<p align="justify">O Randon Forest √© um algoritmo de ensemble baseado em √°rvores de decis√£o. Quando se combina v√°rias √°rvores, temos uma floresta. Ele cria v√°rias √°rvores de decis√£o usando conjuntos de dados aleat√≥rios e, em seguida, combina as previs√µes de cada √°rvore para produzir uma previs√£o final. O Random Forest √© um conjunto de v√°rias √°rvores de decis√£o que trabalham juntas para fazer previs√µes mais precisas. Ao inv√©s de depender de uma √∫nica √°rvore, ele cria m√∫ltiplas √°rvores e combina suas respostas. Isso o torna mais robusto e menos propenso a erros causados por varia√ß√µes nos dados. Ele usa a vota√ß√£o entre √°rvores para prever categorias e a m√©dia das previs√µes para problemas de regress√£o.</p>
<p align="justify">Como funciona?</p>
<p align="justify"><strong>Cria√ß√£o de v√°rias √°rvores de decis√£o ‚Üí</strong> O algoritmo constr√≥i v√°rias √°rvores, cada uma com um conjunto ligeiramente diferente de dados.</p>
<p align="justify"><strong>Cada √°rvore faz uma previs√£o ‚Üí</strong> Quando recebe um novo dado, cada √°rvore d√° um "palpite" sobre a classe correta.</p>
<p align="justify"><strong>Vota√ß√£o das √°rvores (Classifica√ß√£o) ‚Üí</strong> No caso de classifica√ß√£o, cada √°rvore vota e a resposta mais escolhida entre todas √© a decis√£o final.</p>
<p align="justify"><strong>M√©dia das previs√µes (Regress√£o) ‚Üí</strong> Para problemas de regress√£o, o resultado final √© uma m√©dia das previs√µes feitas pelas √°rvores.</p>

<p align="justify">Descri√ß√£o do c√≥digo:</p>

```python
# Separa√ß√£o entre features (X) e target (y)
# Separa√ß√£o de Dados: Os dados s√£o divididos entre X (features) e y (target, no caso a sobreviv√™ncia).
# Dados tratados
# Define X como os dados que foram pr√©-processados no c√≥digo acima apresentado na √°rea de tratamento de dados.
X = preprocessed_df
# Convers√£o da Target: Transformamos "Survived" em 1 e "Deceased" em 0.
# Converte a vari√°vel alvo ("Survived" ‚Üí 1 e "Deceased" ‚Üí 0) para valores num√©ricos.
y = df['Survival_Status'].replace({'Survived': 1, 'Deceased': 0}).infer_objects(copy=False)

```

```python
# Divis√£o do Dataset: O dataset √© dividido em treinamento (80%) e teste (20%) para avaliar o desempenho dos modelos. (Obs. Testamos tamb√©m com outras porcentagens que ser√£o descritas abaixo.)
# Treinamento dos Modelos: Cada modelo recebe os dados de treinamento (X_train, y_train) e aprende padr√µes para fazer previs√µes.
# Divide os dados em: 80% treino ‚Üí usados para ensinar os modelos. 20% teste ‚Üí usados para validar os modelos.
# Garante que a divis√£o seja reprodut√≠vel (os mesmos conjuntos sempre que o c√≥digo for executado).
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Print para informa√ß√£o de quantidade de linhas e colunas sendo usadas em X_train e X_test.
print(f"N√∫mero de linhas em X_train: {X_train.shape[0]}")
print(f"N√∫mero de colunas em X_train: {X_train.shape[1]}")

print(f"N√∫mero de linhas em X_test: {X_test.shape[0]}")
print(f"N√∫mero de colunas em X_test: {X_test.shape[1]}")

```

```python
# 1Ô∏è‚É£ Modelo Random Forest
# Cria um modelo Random Forest com 100 √°rvores. (Obs. Testamos tamb√©m com outras quantidades de √°rvores que ser√£o descritas abaixo.)

# O Random Forest √© um algoritmo baseado em m√∫ltiplas √°rvores de decis√£o. Cada √°rvore aprende um pequeno aspecto dos dados e, no final, todas as √°rvores juntas fazem uma vota√ß√£o para dar uma previs√£o mais robusta.
# üîπ Mais √°rvores = mais estabilidade. Quando o modelo tem poucas √°rvores, ele pode ter mais varia√ß√µes e ser sens√≠vel a mudan√ßas nos dados. Com mais √°rvores, ele generaliza melhor, reduzindo o risco de tomar decis√µes erradas devido a dados espec√≠ficos do treino.

# üîπ Aprimora a precis√£o Geralmente, aumentar o n√∫mero de √°rvores melhora a precis√£o, pois cada √°rvore traz uma perspectiva diferente sobre os dados.

# üîπ Compromisso entre desempenho e tempo de execu√ß√£o Testes pr√°ticos mostram que 100 √°rvores √© um bom n√∫mero para equilibrar qualidade e velocidade.

# Se tivermos milhares de √°rvores, o treinamento pode ficar muito lento sem ganhos significativos na precis√£o.
# O Bagging ocorre quando est√° sendo instanciado um modelo RandomForestClassifier com n_estimators=100, ou seja, 100 √°rvores de decis√£o ser√£o treinadas usando subconjuntos aleat√≥rios do seu dataset.
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)

# Treina o modelo nos dados de treino.
# O Random Forest tamb√©m aplica Feature Selection automaticamente ao treinar o modelo (rf_model.fit(X_train, y_train)).
# Ele seleciona aleatoriamente um subconjunto de atributos para cada √°rvore, reduzindo a depend√™ncia de atributos irrelevantes
rf_model.fit(X_train, y_train)

# Obter import√¢ncia das features (Feature Selection)
feature_importances = rf_model.feature_importances_

# Ordenar e visualizar
indices = np.argsort(feature_importances)[::-1]  # Ordena do maior para o menor
plt.figure(figsize=(30, 20))
plt.title("Import√¢ncia das Features no Random Forest")
plt.bar(range(X_train.shape[1]), feature_importances[indices])
plt.xticks(range(X_train.shape[1]), np.array(all_feature_names)[indices], rotation=90)
plt.show()

# Faz previs√µes no conjunto de teste.
rf_pred = rf_model.predict(X_test)

```

```python

# Matriz de Confus√£o para Random Forest
# Ela compara os valores reais (y_test) com os valores preditos (rf_pred) pelo modelo Random Forest.
cm_rf = confusion_matrix(y_test, rf_pred)
# Cria uma nova √°rea de figura do matplotlib, definindo o tamanho dela: 8 de largura por 6 de altura.
plt.figure(figsize=(8, 6))
# sns.heatmap √© a fun√ß√£o do Seaborn que desenha uma mapa de calor (heatmap).
sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Greens',
            xticklabels=['Deceased', 'Survived'], yticklabels=['Deceased', 'Survived'])
# Define o t√≠tulo do eixo X como "Valores Previstos".
plt.xlabel('Valores Previstos')
# Define o t√≠tulo do eixo Y como "Valores Reais".
plt.ylabel('Valores Reais')
# Define o t√≠tulo do gr√°fico.
plt.title('Matriz de Confus√£o: Random Forest')
# Exibe o gr√°fico na tela.
plt.show()

```

<p align="justify">Comparando com outros modelos XGBoost e Naive Bayes</p>

```python

# 2Ô∏è‚É£ Modelo XGBoost
# Cria um modelo XGBoost.
xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')
# Treina o modelo com os dados de treino.
xgb_model.fit(X_train, y_train)
# Faz previs√µes no conjunto de teste.
xgb_pred = xgb_model.predict(X_test)

# Matriz de Confus√£o para XGBoost
cm_xgb = confusion_matrix(y_test, xgb_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_xgb, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Deceased', 'Survived'], yticklabels=['Deceased', 'Survived'])
plt.xlabel('Valores Previstos')
plt.ylabel('Valores Reais')
plt.title('Matriz de Confus√£o: XGBoost')
plt.show()

```

```python

# 3Ô∏è‚É£ Modelo Naive Bayes
# Cria um modelo Naive Bayes baseado na distribui√ß√£o normal.
nb_model = GaussianNB()
# Treina o modelo com os dados de treino.
nb_model.fit(X_train, y_train)
# Faz previs√µes no conjunto de teste.
nb_pred = nb_model.predict(X_test)

# Matriz de Confus√£o para Naive Bayes
cm_nb = confusion_matrix(y_test, nb_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_nb, annot=True, fmt='d', cmap='Oranges',
            xticklabels=['Deceased', 'Survived'], yticklabels=['Deceased', 'Survived'])
plt.xlabel('Valores Previstos')
plt.ylabel('Valores Reais')
plt.title('Matriz de Confus√£o: Naive Bayes')
plt.show()

```

<p align="justify">Calculando, exibindo as m√©tricas e avaliando os tr√™s modelos.</p>

```python

# Fun√ß√£o para calcular e exibir as m√©tricas
# Avalia√ß√£o dos Modelos: O c√≥digo mede o desempenho dos tr√™s modelos usando as m√©tricas: üîπ Acur√°cia (quantidade de previs√µes corretas) üîπ Precis√£o (qu√£o correto √© quando prev√™ sobreviv√™ncia)
# üîπ Recall (quantos casos de sobreviv√™ncia foram corretamente identificados) üîπ F1-score (m√©dia harm√¥nica entre precis√£o e recall)
# Define uma fun√ß√£o evaluate_model(name, y_test, y_pred), que recebe: name ‚Üí Nome do modelo. // y_test ‚Üí Verdadeiro status de sobreviv√™ncia. // y_pred ‚Üí Previs√µes do modelo.
# Calcula e exibe as m√©tricas de desempenho: Acur√°cia ‚Üí Percentagem de previs√µes corretas. // Precis√£o ‚Üí Qu√£o correto √© quando prev√™ sobreviv√™ncia. // Recall ‚Üí Quantos casos de sobreviv√™ncia foram identificados corretamente.
# F1-score ‚Üí Combina√ß√£o entre precis√£o e recall.
def evaluate_model(name, y_test, y_pred):
    print(f"\nResultados para {name}:")
    print(f"Acur√°cia: {accuracy_score(y_test, y_pred):.4f}")
    print(f"Precis√£o: {precision_score(y_test, y_pred):.4f}")
    print(f"Recall: {recall_score(y_test, y_pred):.4f}")
    print(f"F1-Score: {f1_score(y_test, y_pred):.4f}")

# Avalia√ß√£o dos modelos
# Chama a fun√ß√£o evaluate_model para cada modelo, imprimindo os resultados das m√©tricas.
evaluate_model("Random Forest", y_test, rf_pred)
evaluate_model("XGBoost", y_test, xgb_pred)
evaluate_model("Naive Bayes", y_test, nb_pred)

```

<p align="justify">Resultados:</p>

<p align="center">
  <img src="https://github.com/user-attachments/assets/4a5d2aad-c333-4cf8-a102-3ad7e0d93ef2" alt="image">
</p>

<p align="center">
  <img src="https://github.com/user-attachments/assets/3e135ecc-49fd-4f97-b103-1f64f1183209" alt="image">
</p>


## xgboost:

<p align="justify">XGBoost √© um algoritmo de aprendizado de m√°quina de ensemble que utiliza o princ√≠pio de gradient boosting. Boosting √© uma t√©cnica onde m√∫ltiplos modelos fracos (normalmente √°rvores de decis√£o) s√£o combinados para formar um modelo forte. No gradient boosting, os modelos s√£o adicionados sequencialmente, com cada novo modelo tentando corrigir os erros dos modelos anteriores.</p>

```python
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score
import pandas as pd
```
#### 1 Vari√°vel Alvo:
```python
target = preprocessed_df['Survival_Status_Numeric']
```

#### 2 Vari√°veis Independentes:

```python
independent_vars_multi_xgb = [
    'Age', 'BMI', 'Gender_Female', 'Gender_Male',
    'Smoking_Status_Current', 'Smoking_Status_Never', 'Alcohol_Consumption_High', 'Alcohol_Consumption_Low',
    'Stage_at_Diagnosis_I', 'Stage_at_Diagnosis_II', 'Stage_at_Diagnosis_III', 'Stage_at_Diagnosis_IV',
    'Physical_Activity_Level_High', 'Physical_Activity_Level_Low'
    # Adicione outras vari√°veis que voc√™ deseja incluir
]
X_multi_xgb = preprocessed_df[independent_vars_multi_xgb].dropna()
y_multi_xgb = target[X_multi_xgb.index]
```

#### 3 Dividir os Dados:
```python
X_train_multi_xgb, X_test_multi_xgb, y_train_multi_xgb, y_test_multi_xgb = train_test_split(X_multi_xgb, y_multi_xgb, test_size=0.3, random_state=42)
```

#### 4 Treinar o Modelo XGBoost:
```python
model_multi_xgb = xgb.XGBClassifier(objective='binary:logistic', eval_metric='logloss', use_label_encoder=False, random_state=42)
model_multi_xgb.fit(X_train_multi_xgb, y_train_multi_xgb)
```

#### 5 Avaliar o Modelo:
```python
y_pred_multi_xgb = model_multi_xgb.predict(X_test_multi_xgb)
y_pred_multi_xgb_prob = model_multi_xgb.predict_proba(X_test_multi_xgb)[:, 1]

print("\nAcur√°cia do XGBoost (Multivariada):", accuracy_score(y_test_multi_xgb, y_pred_multi_xgb))
print("\nRelat√≥rio de Classifica√ß√£o do XGBoost (Multivariada):\n", classification_report(y_test_multi_xgb, y_pred_multi_xgb))
print("\nAUC-ROC do XGBoost (Multivariada):", roc_auc_score(y_test_multi_xgb, y_pred_multi_xgb_prob))
```

### Explica√ß√£o Detalhada:

<p align="justify">Este c√≥digo implementa um modelo de aprendizado de m√°quina usando o algoritmo XGBoost para resolver um problema de classifica√ß√£o bin√°ria. O objetivo √© prever o status de sobreviv√™ncia (ou algo similar) com base em um conjunto de vari√°veis independentes. Vou explicar cada se√ß√£o do c√≥digo em detalhes:</p>

#### 1. Importa√ß√£o de Bibliotecas:
```python
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score
import pandas as pd
```

<p align="justify">- import xgboost as xgb: Importa a biblioteca XGBoost, que fornece uma implementa√ß√£o eficiente do algoritmo de gradient boosting. O XGBoost √© conhecido por seu desempenho e velocidade em tarefas de aprendizado de m√°quina.</p>

<p align="justify">- from sklearn.model_selection import train_test_split: Importa a fun√ß√£o train_test_split do scikit-learn. Essa fun√ß√£o √© usada para dividir o conjunto de dados em conjuntos de treinamento e teste, o que √© essencial para avaliar o desempenho do modelo.</p>

- from sklearn.metrics import accuracy_score, classification_report, roc_auc_score: Importa m√©tricas de avalia√ß√£o do scikit-learn.

- accuracy_score: Calcula a acur√°cia da classifica√ß√£o.

- classification_report: Gera um relat√≥rio com m√©tricas como precis√£o, recall e F1-score.

- roc_auc_score: Calcula a √Årea Sob a Curva ROC, √∫til para avaliar o desempenho de classificadores bin√°rios.

- import pandas as pd: Importa a biblioteca pandas, usada para manipular e analisar dados tabulares (DataFrames).
 
#### 2. Vari√°vel Alvo:
```python
- target = preprocessed_df['Survival_Status_Numeric']
```
- target = preprocessed_df['Survival_Status_Numeric']: Define a vari√°vel alvo, que √© a vari√°vel que o modelo tentar√° prever. Nesse caso, a vari√°vel alvo √© 'Survival_Status_Numeric', que representa o status de sobreviv√™ncia. Sup√µe-se que preprocessed_df √© um DataFrame do pandas que cont√©m os dados.

#### 3. Vari√°veis Independentes:

```python
independent_vars_multi_xgb = [
    'Age', 'BMI', 'Gender_Female', 'Gender_Male',
    'Smoking_Status_Current', 'Smoking_Status_Never', 'Alcohol_Consumption_High', 'Alcohol_Consumption_Low',
    'Stage_at_Diagnosis_I', 'Stage_at_Diagnosis_II', 'Stage_at_Diagnosis_III', 'Stage_at_Diagnosis_IV',
    'Physical_Activity_Level_High', 'Physical_Activity_Level_Low'
    # Adicione outras vari√°veis que voc√™ deseja incluir
]
X_multi_xgb = preprocessed_df[independent_vars_multi_xgb].dropna()
y_multi_xgb = target[X_multi_xgb.index]
```
- independent_vars_multi_xgb: Lista dos nomes das colunas que ser√£o usadas como vari√°veis independentes (preditoras). Essas vari√°veis s√£o usadas para prever a vari√°vel alvo.

- X_multi_xgb = preprocessed_df[independent_vars_multi_xgb].dropna(): Cria um novo DataFrame X_multi_xgb contendo apenas as colunas especificadas em independent_vars_multi_xgb. O m√©todo .dropna() remove quaisquer linhas que contenham valores ausentes, garantindo que o modelo seja treinado com dados limpos.

- y_multi_xgb = target[X_multi_xgb.index]: Cria uma nova s√©rie y_multi_xgb que cont√©m os valores da vari√°vel alvo (target) correspondentes aos √≠ndices das linhas em X_multi_xgb. Isso garante que as vari√°veis independentes e a vari√°vel alvo estejam alinhadas ap√≥s a remo√ß√£o de valores ausentes.

#### 4. Divis√£o dos Dados:
```python
X_train_multi_xgb, X_test_multi_xgb, y_train_multi_xgb, y_test_multi_xgb = train_test_split(X_multi_xgb, y_multi_xgb, test_size=0.3, random_state=42)
```
- train_test_split: Divide o conjunto de dados em conjuntos de treinamento e teste.

- X_train_multi_xgb, y_train_multi_xgb: Conjuntos de dados usados para treinar o modelo.

- X_test_multi_xgb, y_test_multi_xgb: Conjuntos de dados usados para avaliar o desempenho do modelo em dados n√£o vistos.

- test_size=0.3: Especifica que 30% dos dados ser√£o usados para teste.

- random_state=42: Define uma semente para o gerador de n√∫meros aleat√≥rios, garantindo que a divis√£o dos dados seja reproduz√≠vel.

#### 5. Treinamento do Modelo XGBoost:
```python
- model_multi_xgb = xgb.XGBClassifier(objective='binary:logistic', eval_metric='logloss', use_label_encoder=False, random_state=42)
model_multi_xgb.fit(X_train_multi_xgb, y_train_multi_xgb)
```
- model_multi_xgb = xgb.XGBClassifier(...): Cria uma inst√¢ncia do classificador XGBoost.

- objective='binary:logistic': Especifica que o problema √© de classifica√ß√£o bin√°ria e usa a fun√ß√£o log√≠stica como fun√ß√£o objetivo.

- eval_metric='logloss': Define a m√©trica de avalia√ß√£o para o desempenho do modelo durante o treinamento como log loss.

- use_label_encoder=False: Desativa o uso do LabelEncoder do scikit-learn para evitar avisos.

- random_state=42: Garante a reprodutibilidade do treinamento do modelo.

- model_multi_xgb.fit(X_train_multi_xgb, y_train_multi_xgb): Treina o modelo XGBoost usando os dados de treinamento.

#### 6. Avaliar o Modelo:
```python
y_pred_multi_xgb = model_multi_xgb.predict(X_test_multi_xgb)
y_pred_multi_xgb_prob = model_multi_xgb.predict_proba(X_test_multi_xgb)[:, 1]

print("\nAcur√°cia do XGBoost (Multivariada):", accuracy_score(y_test_multi_xgb, y_pred_multi_xgb))
print("\nRelat√≥rio de Classifica√ß√£o do XGBoost (Multivariada):\n", classification_report(y_test_multi_xgb, y_pred_multi_xgb))
print("\nAUC-ROC do XGBoost (Multivariada):", roc_auc_score(y_test_multi_xgb, y_pred_multi_xgb_prob))
```
- y_pred_multi_xgb = model_multi_xgb.predict(X_test_multi_xgb): Faz previs√µes de classe para o conjunto de teste.

- y_pred_multi_xgb_prob = model_multi_xgb.predict_proba(X_test_multi_xgb)[:, 1]: Obt√©m as probabilidades previstas da classe positiva para o conjunto de teste.

#### 7. As instru√ß√µes print exibem as m√©tricas de avalia√ß√£o:

- Acur√°cia: A propor√ß√£o de previs√µes corretas.

- Relat√≥rio de Classifica√ß√£o: Inclui precis√£o, recall, F1-score e suporte para cada classe.

- AUC-ROC: A √Årea Sob a Curva Caracter√≠stica de Opera√ß√£o do Receptor, que mede a capacidade do modelo de distinguir entre as classes.
__________

Nesta se√ß√£o, conhecendo os dados e de posse dos dados preparados, √© hora de descrever os algoritmos de aprendizado de m√°quina selecionados para a constru√ß√£o dos modelos propostos. Inclua informa√ß√µes abrangentes sobre cada algoritmo implementado, aborde conceitos fundamentais, princ√≠pios de funcionamento, vantagens/limita√ß√µes e justifique a escolha de cada um dos algoritmos.

Explore aspectos espec√≠ficos, como o ajuste dos par√¢metros livres de cada algoritmo. Lembre-se de experimentar par√¢metros diferentes e principalmente, de justificar as escolhas realizadas.

Como parte da comprova√ß√£o de constru√ß√£o dos modelos, um v√≠deo de demonstra√ß√£o com todas as etapas de pr√©-processamento e de execu√ß√£o dos modelos dever√° ser entregue. Este v√≠deo poder√° ser do tipo _screencast_ e √© imprescind√≠vel a narra√ß√£o contemplando a demonstra√ß√£o de todas as etapas realizadas.

# Avalia√ß√£o dos modelos criados

## M√©tricas utilizadas

Nesta se√ß√£o, as m√©tricas utilizadas para avaliar os modelos desenvolvidos dever√£o ser apresentadas (p. ex.: acur√°cia, precis√£o, recall, F1-Score, MSE etc.). A escolha de cada m√©trica dever√° ser justificada, pois esta escolha √© essencial para avaliar de forma mais assertiva a qualidade do modelo constru√≠do. 

## Discuss√£o dos resultados obtidos

Nesta se√ß√£o, discuta os resultados obtidos por cada um dos modelos constru√≠dos, no contexto pr√°tico em que os dados se inserem, promovendo uma compreens√£o abrangente e aprofundada da qualidade de cada um deles. Lembre-se de relacionar os resultados obtidos ao problema identificado, a quest√£o de pesquisa levantada e estabelecer rela√ß√£o com os objetivos previamente propostos. N√£o deixe de comparar os resultados obtidos por cada modelo com os demais.

# Pipeline de pesquisa e an√°lise de dados

Em pesquisa e experimenta√ß√£o em sistemas de informa√ß√£o, um pipeline de pesquisa e an√°lise de dados refere-se a um conjunto organizado de processos e etapas que um profissional segue para realizar a coleta, prepara√ß√£o, an√°lise e interpreta√ß√£o de dados durante a fase de pesquisa e desenvolvimento de modelos. Esse pipeline √© essencial para extrair _insights_ significativos, entender a natureza dos dados e, construir modelos de aprendizado de m√°quina eficazes. 

# V√≠deo de apresenta√ß√£o da Etapa 03

Nesta se√ß√£o, dever√° ser entregue um v√≠deo onde dever√£o ser descritas todas as etapas realizadas. O v√≠deo, que n√£o tem limite de tempo, dever√° ser apresentado por **todos os integrantes da equipe**, de forma que, cada integrante tenha oportunidade de apresentar o que desenvolveu e as percep√ß√µes obtidas. Alunos que n√£o participarem, ser√£o penalizados na nota final da etapa.

## Observa√ß√µes importantes

Todas as tarefas realizadas nesta etapa dever√£o ser registradas em formato de texto junto com suas explica√ß√µes de forma a apresentar os c√≥digos desenvolvidos e tamb√©m, o c√≥digo dever√° ser inclu√≠do, na √≠ntegra, na pasta "src".
